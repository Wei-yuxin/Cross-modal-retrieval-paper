# Cross-modal-retrieval-paper
Collection of papers on cross modal retrieval.

# paper list

## 2022

- (2022 IEEE Trans)**Adversarial Graph Convolutional Network for Cross-modal Retrieval** [ðŸ“„paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9411880&tag=1) 
- (2022 CVPR)**Cross-modal Prototype Driven Network for Radiology Report Generation** [ðŸ“„paper](https://arxiv.org/abs/2207.04818) 
- (2022)**A Comprehensive Empirical Study of Vision-LanguagePre-trained Model for Supervised Cross-Modal Retrieval** [ðŸ“„paper](https://arxiv.org/abs/2201.02772) [pdf](https://arxiv.org/pdf/2201.02772.pdf)
- (2022 CVPR)**Mutual Quantization for Cross-Modal Search with Noisy Labels** [ðŸ“„paper](https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Mutual_Quantization_for_Cross-Modal_Search_With_Noisy_Labels_CVPR_2022_paper.html) 
- (2022 CVPR)**Integrating Language Guidance into Vision-based Deep Metric Learning** [ðŸ“„paper ](https://openaccess.thecvf.com/content/CVPR2022/html/Roth_Integrating_Language_Guidance_Into_Vision-Based_Deep_Metric_Learning_CVPR_2022_paper.html) [pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Roth_Integrating_Language_Guidance_Into_Vision-Based_Deep_Metric_Learning_CVPR_2022_paper.pdf)
- (2022 ICML)**Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks** [ðŸ“„paper](https://proceedings.mlr.press/v162/wu22d.html)
- (2022 IEEE Trans)**Deep Cross-modal Proxy Hashing** [ðŸ“„paper ](https://ieeexplore.ieee.org/abstract/document/9809792)
- (2022 IEEE Trans)**Category Alignment Adversarial Learning for Cross-modal Retrieval** [paper ](https://ieeexplore.ieee.org/abstract/document/9737126)
- (2022 IEEE Trans)**Deep Supervised Dual Cycle Adversarial Network for Cross-Modal Retrieval** [ðŸ“„paper](https://ieeexplore.ieee.org/abstract/document/9880488)
- (2022 MDPI)**Integrating Adversarial Generative Network with Variational Autoencoders towards Cross-Modal Alignment for Zero-Shot Remote Sensing Image Scene Classification** [ðŸ“„paper](https://www.mdpi.com/2072-4292/14/18/4533) 
- (2022 AAAI)**Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification** [ðŸ“„paper](https://ojs.aaai.org/index.php/AAAI/article/view/20614) 
- (2022 IEEE Trans)**Adversarial Graph Convolutional Network for Cross-Modal Retrieval**  [ðŸ“„paper](https://ieeexplore.ieee.org/document/9411880) 
- (2022 IEEE Trans)**Learning Cross-Modal Common Representations by Privateâ€“Shared Subspaces Separation** [ðŸ“„paper](https://ieeexplore.ieee.org/document/9165187) 

## 2021

- (2021) **HCMSL: Hybrid Cross-modal Similarity Learning for Cross-modal Retrieval** [paper](https://www.researchgate.net/profile/Chengyuan-Zhang-6/publication/344307665_HCMSL_Hybrid_Cross-Modal_Similarity_Learning_for_Cross-Modal_Retrieval/links/5f658e59458515b7cf3eda00/HCMSL-Hybrid-Cross-Modal-Similarity-Learning-for-Cross-Modal-Retrieval.pdf)
- (2021 CVPR) **Cross-modal Center Loss** [pdf](https://arxiv.org/abs/2008.03561) [code](https://github.com/LongLong-Jing/Cross-Modal-Center-Loss)
- (2021 CVPR)**Probabilistic Embeddings for Cross-Modal Retrieval**  [ðŸ“„paper](https://arxiv.org/pdf/2101.05068.pdf)  [ðŸ’»pytorch](https://github.com/naver-ai/pcme)
- (2021 AAAI)**Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval** [ðŸ“„paper](https://www.aaai.org/AAAI21Papers/AAAI-6207.QianS.pdf)
- (2021 AAAI)**Deep Graph-Neighbor Coherence Preserving Network for Unsupervised Cross-Modal  Hashing** [ðŸ“„paper](https://www.aaai.org/AAAI21Papers/AAAI-2796.YuJ.pdf)  [ðŸ’»pytorch](https://github.com/Atmegal/DGCPN)
- (2021)**Discriminative Semantic Transitive Consistency for Cross-Modal Learning** [ðŸ“„paper](https://arxiv.org/pdf/2103.14103.pdf)
- (2021 IEEE Trans)**Augmented Adversarial Training for Cross-Modal Retrieval**  [ðŸ“„paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9057710)   [ðŸ’»TensorFlow](https://github.com/yiling2018/aacr)
- (2021 SIGIR)**PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval** [ðŸ“„paper](https://dl.acm.org/doi/abs/10.1145/3404835.3462867)  
- (2021)**DRSL: Deep Relational Similarity Learning for Cross-modal Retrieval**[ðŸ“„paper](https://www.sciencedirect.com/science/article/abs/pii/S0020025520307684) [ðŸ’»code](https://github.com/wangxu-scu/DRSL)
- (2021 ISPRS)**Robust deep alignment network with remote sensing knowledge graph for zero-shot and generalized zero-shot remote sensing image scene classification** [ðŸ“„paper](https://www.sciencedirect.com/science/article/abs/pii/S092427162100201X)
- (2021 MM)**Zero-shot Cross-modal Retrieval by Assembling AutoEncoder and Generative Adversarial Network** [ðŸ“„paper](https://dl.acm.org/doi/abs/10.1145/3424341)

## 2020

- (2020) **Incomplete Cross-modal Retrieval with Dual-Aligned Variational Autoencoders** [paper](https://dl.acm.org/doi/pdf/10.1145/3394171.3413676)
- (2020) **Modality-specific and shared generative adversarial network for cross-modal retrieval** [paper](https://reader.elsevier.com/reader/sd/pii/S0031320320301382?token=736E808929A780DD58D84AD3E0D38143F9014B5E212C0F89EB433AC585CDAD8913063A0DB4B795EF03F699B439F8F68D&originRegion=us-east-1&originCreation=20210729122016)
- (2020 CVPR)**Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing** [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf) [ðŸ’»github](https://github.com/huhengtong/UKD_CVPR2020)
- (2020 CVPR)**IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval** [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_IMRAM_Iterative_Matching_With_Recurrent_Attention_Memory_for_Cross-Modal_Image-Text_CVPR_2020_paper.pdf)  [ðŸ’»pytorch](https://github.com/HuiChen24/IMRAM)
- (2020 CVPR)**Universal Weighting Metric Learning for Cross-Modal Matching**  [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Universal_Weighting_Metric_Learning_for_Cross-Modal_Matching_CVPR_2020_paper.pdf)  [ðŸ’»pytorch](https://github.com/wayne980/PolyLoss)
- (2020 CVPR)**Cross-Domain Detection via Graph-Induced Prototype Alignment** [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Cross-Domain_Detection_via_Graph-Induced_Prototype_Alignment_CVPR_2020_paper.pdf) 
- (2020 CVPR)**MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model**  [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_MCEN_Bridging_Cross-Modal_Gap_between_Cooking_Recipes_and_Dish_Images_CVPR_2020_paper.pdf)
- (2020  AAAI )**Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification**  [ðŸ“„paper](https://arxiv.org/pdf/1912.07872v1.pdf)
- (2020 ICIP)**Semi-supervised Graph Convolutional Hashing Network For Large-Scale Cross-Modal Retrieval** [ðŸ“„](https://ieeexplore.ieee.org/document/9190641)[paper](https://ieeexplore.ieee.org/document/9190641) [ðŸ’»tensorflow](https://github.com/flyingjohn/Cross_Modal_GCN)
- (2020) **COBRA: Contrastive Bi-Modal Representation Algorithm** [ðŸ“„paper](https://arxiv.org/pdf/2005.03687v2.pdf)  [ðŸ’»pytorch](https://github.com/ovshake/cobra)
- (2020)**DSAN Cross-modal dual subspace learning with adversarial network** [ðŸ“„paper](https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608020X00049/1-s2.0-S0893608020300927/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDtY6e2gwMhYprFF%2FuWMug9oyXUqesdloBb5licXPK%2B3gIhAPe%2BbrhiYB3XasxpRSu7blrYRtviyiIjSJp593hoykiEKoMECI3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMDU5MDAzNTQ2ODY1Igyzy3vdPCSiTjjwB1Mq1wNLY62M9mwnmAcEQVJVIYVqXp1likHEu05%2BfOONWEDjYIfkG%2FQfe3JZV5ItGi9vilV7GX02Pb7arXcVMZcsS9mfcnSMNgfDPDWIyt717oTkV09sVU0FjsGq%2BHvSxNg3QOBXwitkyEmNjFiUSSDQs5QgdFpe4HNCXLcWAx27Mam%2Bo59nyZRso4mYTJeX2vX8QhzjoiW5xB4DN5X8JnyYLM54C0zxLACJ5yjUJkTVqEkvQRgXvlGEggnBGv9XD5RMKAzrcaFksYB39zavnNqxyxLQWBCdvhztgE6%2Fr7fGSpM9FBGb7GV7%2FjK5qNG5FBPPoDJpe2tqp%2B%2Fd%2FY3r5i1U8JwnIF9x3cSUmVN1oaGOdDDTbeTFSa4txlcWflCeaLF9JbPpBK3knRao4swCFzRLB%2Btptevw0uP0jAhTUBL9%2FlB5znXmtBP8MtH3N5guWtwW4KIqDClppUq5IhdkYWyJkDt2KwoHzcTmbiVjXmZStYBpMGEOM58r%2BVsUiHRWOxmQwRZP%2ByB0ca7BHEeG2Vpv%2F0K5p7qcI1NfXSsSVLPmIKVp1yLkdpZf3q0bH6LGLXqSQfOewzzRb5Vkv7ZYEwOc9eRVhrpyisqbsbzeR0aiHOC2qCzoTJH0We0wgrqKiAY6pAEMh2HNWDxn5OR4ceXrXiczikOPa4%2BFyVRvUc0X8xFNhV%2B7ofoDJeZ6L2kP30qfhjYOw1pwGvD2LjZMlLbpwF4xy3TDUBpv0QGXN6dPkt2ACJ41MVr%2BxU%2BEHKttK8MSg%2BAFfbp7%2FQnILfWU1lnt%2Bu3Ve%2FbIPsPS3Memzu%2BUep79aUbyOS3KUDmsl003RU3OVVz60e4mMkXMi9OJN%2F7yzYbY0DUGWw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210729T130555Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7ASUQQ4G%2F20210729%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b5354378646b05d3899b95fb99ecc616bee9894081981e4ba7da2fa42c459d1c&hash=778f9e631ba699b700bd87b3178b0cecade52653c7d9e17266a3c368d7af0f2f&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608020300927&tid=spdf-7b1b9899-61c0-4801-ad3c-7876ed90e6ff&sid=d2e470c35fbb514c7c3bda46bb145f29400dgxrqa&type=client)
- (2020)**CMPD: Using Cross Memory Network With Pair Discrimination for Image-Text Retrieval**  [ðŸ“„paper](http://cgcad.thss.tsinghua.edu.cn/liuyushen/main/pdf/LiuYS_TCSVT2021.pdf)
- (2020 CVPR)**Universal Weighting Metric Learning for Cross-Modal Matching** [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Wei_Universal_Weighting_Metric_Learning_for_Cross-Modal_Matching_CVPR_2020_paper.html) [ðŸ’»pytorch](https://github.com/wayne980/PolyLoss) 
- (2020 ICME)**learning class prototypes via anisotropic combination of aligned modalities for few-shot learning** [ðŸ“„paper](https://readpaper.com/paper/3034860057)
- (2020) **Representation separation adversarial networks for cross-modal retrieval** [ðŸ“„paper](https://link.springer.com/article/10.1007/s11276-020-02382-4) 
- (2020)**Learning Disentangled Latent Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach** [ðŸ“„paper](https://arxiv.org/abs/2012.00682)
- (2020 PAMI) **Joint Feature Synthesis and Embedding: Adversarial Cross-Modal Retrieval Revisited** [ðŸ“„paper](https://ieeexplore.ieee.org/document/9296975) [ðŸ’»code](https://github.com/CFM-MSG/Code_JFSE)
- (2020 KDD)**Vulnerability vs. Reliability: Disentangled Adversarial Examples for Cross-Modal Learning**  [ðŸ“„paper](https://dl.acm.org/doi/10.1145/3394486.3403084)

## 2019

- (2019 CVPR) **Deep Supervised Cross-modal Retrieval** [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhen_Deep_Supervised_Cross-Modal_Retrieval_CVPR_2019_paper.pdf)   [ðŸ’»ï¸Žpytorch](https://github.com/penghu-cs/DSCMR)
- (2019 SIGIR)**Scalable Deep Multimodal Learning for Cross-Modal Retrieval**[ðŸ“„paper](https://liangli-zhen.github.io/papers/SIGIR2019_Scalable_Deep_Multimodal_Learning_for_Cross-Modal_Retrieval.pdf) [ðŸ’»ï¸Žpytorch](https://github.com/penghu-cs/SDML)
- (2019 WWW)**Deep adversarial metric learning for cross-modal retrieval** [ðŸ“„paper](https://link.springer.com/content/pdf/10.1007/s11280-018-0541-x.pdf)
- (2019 CVPR)**Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval**  [ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf)  [ðŸ’»pytorch](https://github.com/yalesong/pvse)
- (2019 ICDAR)**Cross-Modal Prototype Learning for Zero-Shot Handwriting Recognition** 
- (2019 IJCAI)**Graph Convolutional Network Hashing for Cross-Modal Retrieval** [ðŸ“„paper](https://www.ijcai.org/proceedings/2019/138) [ðŸ’»code](https://github.com/DeXie0808/GCH) 
- (2019 CVPR)**Multi-Label Image Recognition with Graph Convolutional Networks**[ðŸ“„paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Multi-Label_Image_Recognition_With_Graph_Convolutional_Networks_CVPR_2019_paper.html) [ðŸ’»code](https://github.com/megvii-research/ML-GCN)  
- (2019 MM)**Learning Disentangled Representation for Cross-Modal Retrieval with Deep Mutual Information Estimation** [ðŸ“„paper](https://dl.acm.org/doi/10.1145/3343031.3351053) 
- (2019 NeurIPS)**Cross-Modal Learning with Adversarial Samples** [ðŸ“„paper](https://proceedings.neurips.cc/paper/2019/hash/d384dec9f5f7a64a36b5c8f03b8a6d92-Abstract.html)

2018

- (2018 CVPR)**Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models**  [ðŸ“„paper](https://arxiv.org/pdf/1711.06420.pdf)
- (2018 CVPR)**Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval** [ðŸ“„paper](https://arxiv.org/pdf/1804.01223.pdf)  [ðŸ’»ï¸ŽTensorFlow](https://github.com/lelan-li/SSAH) [ðŸ’»ï¸Žpytorch](https://github.com/haitao-hub-stu/SSAH)
- (2018 NeurIPS)**Image-to-image translation for cross-domain disentanglement**[ðŸ“„](https://proceedings.neurips.cc/paper/2018/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html) [paper](https://proceedings.neurips.cc/paper/2018/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html) [ðŸ’»Tensorflow](https://github.com/agonzgarc/cross-domain-disen)

2017

- (2017 ACM MM) **Adversarial Cross-Modal Retrieval** [ðŸ“„paper](https://cfm.uestc.edu.cn/~yangyang/papers/acmr.pdf)  [ðŸ’»ï¸ŽTensorFlow](https://github.com/sunpeng981712364/ACMR_demo)
- (2017) **CCL: Cross-modal Correlation Learning with Multi-grained Fusion by Hierarchical Network** [ðŸ“„paper](https://arxiv.org/pdf/1704.02116.pdf) [ðŸ’»ï¸Žcode](https://github.com/PKU-ICST-MIPL/CCL_TMM2018)
- (2017 CVPR)**Deep Cross-Modal Hashing** [ðŸ“„paper](https://arxiv.org/pdf/1602.02255.pdf) [ðŸ’»ï¸Žpytorch](https://github.com/WendellGul/DCMH) [ðŸ’»ï¸ŽTensorFlow](https://github.com/jiangqy/DCMH-CVPR2017)

2016

- (2016)**On Deep Multi-View Representation Learning: Objectives and Optimization** [ðŸ“„paper](https://arxiv.org/pdf/1602.01024.pdf)
- (2016 IJCAI)**Cross-Media Shared Representation by Hierarchical Learning with Multiple Deep Networks** [ðŸ“„paper](https://www.ijcai.org/Proceedings/16/Papers/541.pdf)   [ðŸ’»ï¸Žcode](https://github.com/PKU-ICST-MIPL/CMDN_IJCAI2016)

2014

- (2014 IEEE Trans)**Learning Cross-Media Joint Representation With Sparse and Semisupervised Regularization** [ðŸ’»ï¸Žcode](https://github.com/PKU-ICST-MIPL/JRL_TCSVT2014)
- (2014)**Cross-modal Retrieval with Correspondence Autoencoder** [ðŸ“„paper](https://people.cs.clemson.edu/~jzwang/1501863/mm2014/p7-feng.pdf)

2013

- (2013)**Deep Canonical Correlation Analysis** [ðŸ“„paper](http://proceedings.mlr.press/v28/andrew13.pdf) [ðŸ’»ï¸Žpytorch](https://github.com/Michaelvll/DeepCCA)

2012

- (2012)**Multi-view Discriminant Analysis** [ðŸ“„paper](https://link.springer.com/content/pdf/10.1007/978-3-642-33718-5_58.pdf)

1992

- (1992)**Relations between two sets of variates** [paper](https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_14)

# dataset

1. Pascal Sentence [link](https://vision.cs.uiuc.edu/pascal-sentences/)
2. Wikipedia [link](http://www.svcl.ucsd.edu/projects/crossmodal/)
3. NUS-WIDE [link](https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/NUS-WIDE.html)
4. XMedia & XMediaNet [link](http://59.108.48.34/tiki/XMediaNet/)
5. FLICKR [link](http://shannon.cs.illinois.edu/DenotationGraph/)
6. mirflickr [link](https://press.liacs.nl/mirflickr/mirdownload.html)
7. coco [link](https://cocodataset.org/#download)



